WEBVTT

1
00:00:00.000 --> 00:00:10.000
[MUSIC]

2
00:00:14.769 --> 00:00:18.642
Now that you've had the chance to
implement a baseline environmental

3
00:00:18.642 --> 00:00:22.778
perception stack, we present to you
our solution for the final project for

4
00:00:22.778 --> 00:00:23.579
this course.

5
00:00:24.690 --> 00:00:28.320
In this video, we will walk through
the solutions required to complete

6
00:00:28.320 --> 00:00:30.800
all the graded functions
in the assessment.

7
00:00:30.800 --> 00:00:34.580
Let's begin with the first task required
to complete the final assessment,

8
00:00:34.580 --> 00:00:36.990
ground plane estimation.

9
00:00:36.990 --> 00:00:39.080
To estimate the ground plane, the x,

10
00:00:39.080 --> 00:00:43.750
y, and z coordinates of every pixel
in the scene need to be estimated.

11
00:00:43.750 --> 00:00:47.330
We start by first implementing the x,
y from depth function.

12
00:00:48.350 --> 00:00:53.260
This function takes as input the depth map
and the calibration metrics, k, provided

13
00:00:53.260 --> 00:00:59.530
by the data set handle, and outputs the x,
y coordinates of every pixel in the scene.

14
00:00:59.530 --> 00:01:03.740
The z coordinate of every pixel is drawn
from the depth map value itself so

15
00:01:03.740 --> 00:01:06.150
the function does not need
to compute this quantity.

16
00:01:07.600 --> 00:01:09.550
First, we grab the focal length and

17
00:01:09.550 --> 00:01:12.050
the camera center parameters
from the calibration matrix.

18
00:01:13.290 --> 00:01:18.360
Then, we create a grid of u and v
coordinates for every pixel in the image.

19
00:01:18.360 --> 00:01:22.340
It is also possible to simply iterate
over every pixel in the depth map, but

20
00:01:22.340 --> 00:01:25.526
the array structure simplifies storage and
processing instructions.

21
00:01:25.526 --> 00:01:30.572
Third, we use NumPy's broadcasting
functionality to estimate and

22
00:01:30.572 --> 00:01:35.270
return x and y coordinates of every
pixel in the scene over the u,

23
00:01:35.270 --> 00:01:40.584
v grid using the equations provided
in the Jupyter Notebook and videos.

24
00:01:40.584 --> 00:01:44.773
To test the function, we use a few
test points in frame zero, and

25
00:01:44.773 --> 00:01:48.747
notice that we arrive at
the correct results for each point.

26
00:01:48.747 --> 00:01:52.790
Now that we have the x, y, and
z coordinates of every pixel in the scene,

27
00:01:52.790 --> 00:01:54.114
we can get the x, y, and

28
00:01:54.114 --> 00:01:59.570
z coordinates of pixels belonging to the
road using semantic segmentation outputs.

29
00:01:59.570 --> 00:02:01.260
The code for this is provided for you.

30
00:02:02.450 --> 00:02:06.330
After that, we implement RANSAC to
estimate the plain parameters in

31
00:02:06.330 --> 00:02:09.910
the graded function RANSAC plane fit.

32
00:02:09.910 --> 00:02:12.158
The function takes as input the x, y,

33
00:02:12.158 --> 00:02:15.010
z point coordinates of pixels
belonging to the road.

34
00:02:16.220 --> 00:02:20.680
The first step of RANSAC requires
setting the termination criteria.

35
00:02:20.680 --> 00:02:23.280
We set the maximum number
of iterations to 100,

36
00:02:23.280 --> 00:02:26.860
the minimum number of inliers to 45,000,
and

37
00:02:26.860 --> 00:02:32.470
the distance threshold of a point to
be considered in inlier to 0.1 meters.

38
00:02:32.470 --> 00:02:35.260
Then we begin the main RANSAC loop.

39
00:02:35.260 --> 00:02:40.770
Inside the loop, we first choose 15 points
at random from the input to the function.

40
00:02:40.770 --> 00:02:45.325
These 15 points are used to compute
a plane model using the provided function

41
00:02:45.325 --> 00:02:47.340
computer_plane.

42
00:02:47.340 --> 00:02:51.880
We then get the number of inliers using
the distance to the plane threshold.

43
00:02:51.880 --> 00:02:55.660
And check if the number of inliers is
greater than the number of inliers in all

44
00:02:55.660 --> 00:02:58.240
previous iterations to
update the inliers set.

45
00:02:59.410 --> 00:03:03.850
Finally, we check our minimum number
of inliers termination criteria

46
00:03:03.850 --> 00:03:07.679
to terminate if the number of inliers
is greater than the required threshold.

47
00:03:08.970 --> 00:03:11.630
The function returns
a recomputed plane model

48
00:03:11.630 --> 00:03:14.070
using all the points in
the best inlier set.

49
00:03:15.320 --> 00:03:16.910
We now test our function and

50
00:03:16.910 --> 00:03:20.480
conclude that we get values very
close to the expected results.

51
00:03:21.670 --> 00:03:25.520
Now that the ground plane has been
estimated, we move on to lane boundary

52
00:03:25.520 --> 00:03:29.349
estimation, again, using the output
from semantic segmentation.

53
00:03:30.580 --> 00:03:35.210
As described in the Jupyter Notebook, the
first step is to determine lane boundary

54
00:03:35.210 --> 00:03:40.820
proposals by detecting all lines
belonging to suspected lane boundaries.

55
00:03:40.820 --> 00:03:45.056
We implement this within
the estimate lane lines function.

56
00:03:45.056 --> 00:03:48.910
This function takes as an input
the semantic segmentation

57
00:03:48.910 --> 00:03:51.040
output from the data handler.

58
00:03:51.040 --> 00:03:56.040
And creates a new segmentation image
containing output of the classes belonging

59
00:03:56.040 --> 00:04:00.580
to lane boundaries, such as lane markings,
curves, and sidewalks.

60
00:04:01.580 --> 00:04:05.568
To estimate lane line proposals,
we can apply Canny edge detection,

61
00:04:05.568 --> 00:04:08.920
followed by Hough
transform line estimation.

62
00:04:08.920 --> 00:04:14.100
The output is then reshaped to guarantee
N x 4 output tensor dimensions.

63
00:04:15.120 --> 00:04:17.510
Visualizing the output lane proposals,

64
00:04:17.510 --> 00:04:21.910
we can see that the boundary
proposals contain horizontal lines.

65
00:04:21.910 --> 00:04:25.300
Furthermore, there are 16
output line proposals.

66
00:04:25.300 --> 00:04:28.580
However, looking at the image,
we can see only eight.

67
00:04:28.580 --> 00:04:32.430
This means that there are redundant
lines that need to be merged.

68
00:04:32.430 --> 00:04:33.100
Merging and

69
00:04:33.100 --> 00:04:37.390
filtering need to be implemented
in the merge_lane_lines function.

70
00:04:37.390 --> 00:04:41.460
The function takes as an input the lane
proposals estimated by the previous

71
00:04:41.460 --> 00:04:46.000
function, filters out horizontal
lines based on their slope, and

72
00:04:46.000 --> 00:04:49.460
then merges the remaining lines based
on their slopes and intercepts.

73
00:04:50.830 --> 00:04:53.690
We define similarity thresholds for
the slope and

74
00:04:53.690 --> 00:04:57.380
the intercepts to decide
which line should be merged.

75
00:04:57.380 --> 00:05:02.240
In addition, horizontal lines in
the image space tend to have zero slope.

76
00:05:02.240 --> 00:05:06.790
And so we determine a minimum slope
threshold under which lines are considered

77
00:05:06.790 --> 00:05:10.480
too slanted to be part
of our lane boundaries.

78
00:05:10.480 --> 00:05:14.350
The first step of implementing this
function is to compute this slope and

79
00:05:14.350 --> 00:05:17.800
the intercept of all
the lines in the input.

80
00:05:17.800 --> 00:05:21.500
Second, we filter out lines based
on the minimum slope threshold

81
00:05:21.500 --> 00:05:24.460
to eliminate horizontal lines.

82
00:05:24.460 --> 00:05:28.633
Third, we cluster the line using
the slope and intercept thresholds.

83
00:05:28.633 --> 00:05:32.416
There are multiple ways to perform this
clustering that will result in the same

84
00:05:32.416 --> 00:05:33.770
solution.

85
00:05:33.770 --> 00:05:35.480
For each line in the set of lines,

86
00:05:35.480 --> 00:05:40.005
we identified all other lines in the set
that fall within a slope threshold, and

87
00:05:40.005 --> 00:05:44.660
then checked that the intercepts fall
within an intercept threshold as well.

88
00:05:44.660 --> 00:05:48.290
Any lines that meet both conditions
are clustered and removed from the set.

89
00:05:50.130 --> 00:05:54.290
Finally, we merge the clustered
proposals to get one line per cluster

90
00:05:54.290 --> 00:05:57.200
by computing the cluster's mean,
slope, and intercept.

91
00:05:58.450 --> 00:06:00.710
Visualizing the output of our function,

92
00:06:00.710 --> 00:06:05.239
we can see that each lane boundary has
one line estimate associated with it.

93
00:06:06.540 --> 00:06:11.070
Finally, we extend the lane boundary
to the extent of the drivable space

94
00:06:11.070 --> 00:06:15.260
by implementing the provided function,
extrapolate lines, and

95
00:06:15.260 --> 00:06:18.070
find the closest lines to
the center of the current lane.

96
00:06:19.340 --> 00:06:22.869
The result is the left and
right lane boundaries of the current lane.

97
00:06:24.480 --> 00:06:27.400
The final component required
to finish this assessment

98
00:06:27.400 --> 00:06:32.330
is the computation of the distance to
impact with every object in the scene

99
00:06:32.330 --> 00:06:34.927
using the output from
the object-detection algorithms.

100
00:06:36.450 --> 00:06:41.340
However, the specific algorithm that
provides this output is not very precise.

101
00:06:41.340 --> 00:06:45.310
As such, before estimating the depth
of the obstacles in the scene,

102
00:06:45.310 --> 00:06:48.670
the output of the object
detection needs to be filtered

103
00:06:48.670 --> 00:06:50.750
using the output from
semantics segmentation.

104
00:06:51.860 --> 00:06:56.660
This is performed through the filter
detections by segmentation function.

105
00:06:56.660 --> 00:06:59.810
This function takes as an input,
the initial detections and

106
00:06:59.810 --> 00:07:01.680
the output from
the semantics segmentation.

107
00:07:02.750 --> 00:07:05.300
It then iterates over every detection and

108
00:07:05.300 --> 00:07:10.130
crops the segmentation output to pixels
located inside the detection bounding box.

109
00:07:11.500 --> 00:07:15.490
The number of pixels with the same
category as the detection output is

110
00:07:15.490 --> 00:07:18.780
counted, and
then normalized by the bounding box area.

111
00:07:20.195 --> 00:07:24.160
Bounding boxes are finally filtered
out if the computed normalized count

112
00:07:24.160 --> 00:07:26.680
is less than a threshold of 0.3.

113
00:07:26.680 --> 00:07:29.630
This means that we
eliminate a bounding box if

114
00:07:29.630 --> 00:07:34.505
less than a third of the bounding box area
is occupied by the predicted class pixels.

115
00:07:34.505 --> 00:07:39.260
Visualizing the remaining bounding boxes,
we can see that

116
00:07:39.260 --> 00:07:43.630
only the one fully containing a car
remains after the filtering process.

117
00:07:44.640 --> 00:07:49.360
Finally, we need to write up a function
called find_min_distance_to_detection

118
00:07:49.360 --> 00:07:52.790
to get the minimum distance to impact
with every object in the scene.

119
00:07:54.180 --> 00:07:58.740
This function takes as an input the
filtered detections along with the x, y,

120
00:07:58.740 --> 00:08:02.440
and z coordinates of all
pixels in the image.

121
00:08:02.440 --> 00:08:03.920
During it's first step,

122
00:08:03.920 --> 00:08:08.290
the distance is computed to each
pixel in the filtered detection.

123
00:08:08.290 --> 00:08:11.400
Then the minimum distance in
each detection is computed to be

124
00:08:11.400 --> 00:08:12.319
returned by the function.

125
00:08:14.000 --> 00:08:17.560
Testing our function demonstrates that
we got the correct minimum distance

126
00:08:17.560 --> 00:08:18.090
to impact.

127
00:08:19.100 --> 00:08:23.800
This completes our discussion of the steps
needed to complete the final assessment.

128
00:08:23.800 --> 00:08:28.160
Before we conclude, I recommend you take
the time to compare the approaches we

129
00:08:28.160 --> 00:08:32.450
presented for tackling these tasks
versus what you have designed.

130
00:08:32.450 --> 00:08:33.535
Let us know how you did it.

131
00:08:33.535 --> 00:08:37.357
Congratulations, you have now
completed the final assessment for

132
00:08:37.357 --> 00:08:40.860
the third course in our
self-driving cars specialization.

133
00:08:40.860 --> 00:08:44.429
You should now be able to produce baseline
perceptions tasks that are capable of

134
00:08:44.429 --> 00:08:48.705
guiding this self-driving car to
drive safely in its environment.

135
00:08:48.705 --> 00:08:52.708
This is a critical skill for
aspiring self-driving car engineers, so

136
00:08:52.708 --> 00:08:54.049
keep up the great work.

137
00:08:54.049 --> 00:08:57.621
Remember that there is always more to
learn in this ever changing field.

138
00:08:57.621 --> 00:09:06.371
[MUSIC]